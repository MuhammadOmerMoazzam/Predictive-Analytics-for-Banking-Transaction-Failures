{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Based Transaction Failure Prediction - Model Training\n",
    "\n",
    "This notebook demonstrates the process of training machine learning models to predict transaction failures using the developed modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, project_root)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Import our custom modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_transaction_data, preprocess_data, prepare_features_target, split_data, scale_features\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_trainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelTrainer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import our custom modules\n",
    "from src.model.data_processor import load_transaction_data, preprocess_data, prepare_features_target, split_data, scale_features\n",
    "from src.model.model_trainer import ModelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "Since we don't have real transaction data yet, we'll generate some sample data that mimics the structure of financial transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample transaction data\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "data = {\n",
    "    'transaction_id': range(1, n_samples + 1),\n",
    "    'transaction_amount': np.random.lognormal(mean=3, sigma=1.5, size=n_samples),\n",
    "    'account_balance': np.random.lognormal(mean=4, sigma=1, size=n_samples),\n",
    "    'time_of_day': np.random.randint(0, 24, size=n_samples),\n",
    "    'day_of_week': np.random.randint(0, 7, size=n_samples),\n",
    "    'merchant_category': np.random.choice(['grocery', 'gas', 'retail', 'restaurant', 'online'], size=n_samples),\n",
    "    'transaction_type': np.random.choice(['debit', 'credit'], size=n_samples),\n",
    "    'location_risk_score': np.random.uniform(0, 1, size=n_samples),\n",
    "    'historical_failure_rate': np.random.beta(2, 8, size=n_samples),\n",
    "}\n",
    "\n",
    "# Create a synthetic target variable based on some logical rules\n",
    "failure_prob = (\n",
    "    0.1 +  # Base failure rate\n",
    "    0.2 * (data['transaction_amount'] > 1000) +  # High amount = higher risk\n",
    "    0.1 * (data['location_risk_score'] > 0.8) +  # High risk location = higher risk\n",
    "    0.1 * (data['historical_failure_rate'] > 0.5)  # High historical failure = higher risk\n",
    ")\n",
    "\n",
    "data['transaction_failure'] = np.random.binomial(1, failure_prob, size=n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Generated {len(df)} sample transactions\")\n",
    "print(f\"Failure rate: {df['transaction_failure'].mean():.2%}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Sample Data\n",
    "\n",
    "Save the generated sample data to a CSV file for use in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save the sample data\n",
    "sample_data_path = os.path.join('data', 'sample_transaction_data.csv')\n",
    "df.to_csv(sample_data_path, index=False)\n",
    "print(f\"Sample data saved to {sample_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "Use our data processor module to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using our module\n",
    "df_loaded = load_transaction_data(sample_data_path)\n",
    "print(f\"Loaded {len(df_loaded)} transactions\")\n",
    "\n",
    "# Preprocess data\n",
    "df_processed, label_encoders = preprocess_data(df_loaded)\n",
    "print(f\"Processed data shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Target\n",
    "\n",
    "Separate the features from the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X, y = prepare_features_target(df_processed, 'transaction_failure')\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Features\n",
    "\n",
    "Scale the features for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n",
    "print(\"Features scaled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "Train multiple models using our model trainer module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train models\n",
    "trainer = ModelTrainer()\n",
    "trainer.train_models(X_train_scaled, y_train)\n",
    "print(\"Models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "\n",
    "Evaluate the performance of all trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "best_model_name = trainer.evaluate_models(X_test_scaled, y_test)\n",
    "print(f\"Best model based on F1 score: {best_model_name}\")\n",
    "\n",
    "# Print detailed scores\n",
    "scores = trainer.get_model_scores()\n",
    "for model_name, metrics in scores.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Best Model\n",
    "\n",
    "Save the best performing model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = os.path.join('models', 'best_transaction_model.pkl')\n",
    "trainer.save_model(best_model_name, model_path)\n",
    "print(f\"Best model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have successfully completed the model training process:\n",
    "1. Generated sample transaction data\n",
    "2. Loaded and preprocessed the data\n",
    "3. Trained multiple ML models (Logistic Regression, Random Forest, Naive Bayes)\n",
    "4. Evaluated the models based on accuracy, precision, recall, and F1 score\n",
    "5. Saved the best performing model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
